# -*- coding: utf-8 -*-
"""CapstoneDSAI_PS3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SUqAcxH0xiin3KLJeD01L38hhk8_sJF2
"""

'''
from google.colab import drive
drive.mount('/content/drive')
'''

'''
https://drive.google.com/uc?id=REPLACE_WITH_FILE_ID&export=download

https://drive.google.com/uc?id=1MMnZ1hc8xSI1W_8oAN3Zse_3k3OxRhMx&export=download

'''

import pandas as pd

# Load the CSV file
file_path = '/content/drive/MyDrive/DSAI_Capstone/Reviews.csv'
reviews_df = pd.read_csv(file_path, sep=',')

# Display the first few rows of the dataframe
reviews_df.head()

reviews_df.shape

reviews_df.info()

reviews_df.isnull().sum()

reviews_df.dropna(inplace=True)

reviews_df.shape

reviews_df.drop_duplicates(inplace=True)

reviews_df.shape

reviews_df.describe()

reviews_df['Summary'].value_counts()

reviews_df['UserId'].nunique()

reviews_df['ProductId'].nunique()

reviews_df['Text'].nunique()

reviews_df['Text'].duplicated().sum()

reviews_df[['Text', 'UserId', 'ProductId']].duplicated().sum()

reviews_df[['Text', 'UserId', 'Score', 'ProfileName', 'Time']].value_counts().head(10)

reviews_df.duplicated(subset=['Text', 'UserId', 'Score', 'ProfileName', 'Time']).sum()



reviews_df.shape

reviews_df.drop_duplicates(subset=['Text', 'UserId', 'Score', 'ProfileName', 'Time'], inplace=True)

reviews_df.shape

reviews_df.drop_duplicates(subset=['Text'], inplace=True)

reviews_df.shape

reviews_df['Text'].nunique()





reviews_df_cleaned=reviews_df.copy()

# Convert the 'Time' column to datetime format
reviews_df_cleaned['Time'] = pd.to_datetime(reviews_df_cleaned['Time'], unit='s')

# Display the first few rows to verify the conversion
reviews_df_cleaned[['Time']].head()

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string


# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize lemmatizer and stop words
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Function to preprocess text
def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stop words and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]
    # Join tokens back into a single string
    return ' '.join(tokens)

# Apply the preprocessing to the 'Text' and 'Summary' columns
reviews_df_cleaned['Processed_Text'] = reviews_df_cleaned['Text'].apply(preprocess_text)
reviews_df_cleaned['Processed_Summary'] = reviews_df_cleaned['Summary'].apply(preprocess_text)

# Display the first few rows to verify the changes
reviews_df_cleaned[['Time', 'Processed_Text', 'Processed_Summary']].head()

import matplotlib.pyplot as plt
import seaborn as sns

# Basic Statistics
basic_stats = reviews_df_cleaned.describe()

# Distribution of Scores
plt.figure(figsize=(10, 6))
sns.histplot(reviews_df_cleaned['Score'], bins=5, kde=True)
plt.title('Distribution of Product Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()

# Review Length Analysis
reviews_df_cleaned['Review_Length'] = reviews_df_cleaned['Processed_Text'].apply(len)
reviews_df_cleaned['Summary_Length'] = reviews_df_cleaned['Processed_Summary'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(reviews_df_cleaned['Review_Length'], bins=50, kde=True)
plt.title('Distribution of Review Lengths')
plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(reviews_df_cleaned['Summary_Length'], bins=50, kde=True)
plt.title('Distribution of Summary Lengths')
plt.xlabel('Summary Length')
plt.ylabel('Frequency')
plt.show()

# Helpfulness Analysis
reviews_df_cleaned['Helpfulness_Ratio'] = reviews_df_cleaned['HelpfulnessNumerator'] / (reviews_df_cleaned['HelpfulnessDenominator'] + 1e-5)

plt.figure(figsize=(10, 6))
sns.histplot(reviews_df_cleaned['Helpfulness_Ratio'], bins=50, kde=True)
plt.title('Distribution of Helpfulness Ratios')
plt.xlabel('Helpfulness Ratio')
plt.ylabel('Frequency')
plt.show()

# Time-Based Analysis
reviews_df_cleaned['Year'] = reviews_df_cleaned['Time'].dt.year
yearly_review_counts = reviews_df_cleaned.groupby('Year').size()
yearly_average_score = reviews_df_cleaned.groupby('Year')['Score'].mean()

plt.figure(figsize=(10, 6))
yearly_review_counts.plot(kind='bar')
plt.title('Number of Reviews per Year')
plt.xlabel('Year')
plt.ylabel('Number of Reviews')
plt.show()

plt.figure(figsize=(10, 6))
yearly_average_score.plot(kind='line', marker='o')
plt.title('Average Review Score per Year')
plt.xlabel('Year')
plt.ylabel('Average Score')
plt.show()



!pip install nltk # Make sure nltk is installed
import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initialize the VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to get sentiment scores
def get_sentiment(text):
    sentiment_scores = sia.polarity_scores(text)
    return sentiment_scores

# Apply the function to the 'Processed_Text' column
reviews_df_cleaned['Sentiment_Scores'] = reviews_df_cleaned['Processed_Text'].apply(get_sentiment)

# Extract compound scores to determine overall sentiment
reviews_df_cleaned['Compound_Score'] = reviews_df_cleaned['Sentiment_Scores'].apply(lambda x: x['compound'])

# Classify sentiment based on compound score
def classify_sentiment(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

reviews_df_cleaned['Sentiment'] = reviews_df_cleaned['Compound_Score'].apply(classify_sentiment)

# Display the first few rows to verify
reviews_df_cleaned[['Processed_Text', 'Sentiment_Scores', 'Compound_Score', 'Sentiment']].head()

reviews_df_cleaned['Sentiment'].value_counts()



from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Vectorize the text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X = tfidf_vectorizer.fit_transform(reviews_df_cleaned['Processed_Text'])
y = reviews_df_cleaned['Sentiment']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Logistic Regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = log_reg.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))





# Display dataset overview
print(reviews_df_cleaned.info())
print(reviews_df_cleaned.head())

# Display number of null values removed
initial_row_count = 568454  # Initial row count provided
final_row_count = reviews_df_cleaned.shape[0]
null_values = 53  # Number of null values removed
print(f"Number of null values removed: {null_values}")

# Display number of duplicate entries removed
duplicates_removed = initial_row_count - null_values - reviews_df_cleaned.shape[0]
print(f"Number of duplicate entries removed: {duplicates_removed}")

# Display sample of preprocessed text
print(reviews_df_cleaned[['Processed_Text', 'Processed_Summary']].head())

# Distribution of review scores (save or display)
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(reviews_df_cleaned['Score'], kde=True, bins=5)
plt.title('Distribution of Review Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.savefig('distribution_of_review_scores.png')
plt.show()

# Trends in review scores over time (save or display)
reviews_df_cleaned['Year'] = reviews_df_cleaned['Time'].dt.year
average_score_per_year = reviews_df_cleaned.groupby('Year')['Score'].mean()

plt.figure(figsize=(10, 6))
plt.plot(average_score_per_year.index, average_score_per_year.values, marker='o')
plt.title('Average Review Score per Year')
plt.xlabel('Year')
plt.ylabel('Average Score')
plt.savefig('average_review_score_per_year.png')
plt.show()

# Distribution of helpfulness ratios (save or display)
plt.figure(figsize=(10, 6))
sns.histplot(reviews_df_cleaned['Helpfulness_Ratio'], kde=True, bins=50)
plt.title('Distribution of Helpfulness Ratios')
plt.xlabel('Helpfulness Ratio')
plt.ylabel('Frequency')
plt.xlim(0, 3)
plt.savefig('distribution_of_helpfulness_ratios.png')
plt.show()

# Sample of sentiment scores and classifications
print(reviews_df_cleaned[['Processed_Text', 'Sentiment_Scores', 'Compound_Score', 'Sentiment']].head())

# Distribution of sentiments
sentiment_distribution = reviews_df_cleaned['Sentiment'].value_counts()
print(sentiment_distribution)

# Details on TF-IDF vectorization
print(f"Number of features: {X.shape[1]}")
print("Example feature names:", tfidf_vectorizer.get_feature_names_out()[:10])

# Train/test split details
print(f"Train set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# Model evaluation metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Original indices of the test set
test_indices = y_test.index

# Fetch the corresponding processed text using the original indices
example_predictions = pd.DataFrame({
    'Processed_Text': reviews_df_cleaned.loc[test_indices, 'Processed_Text'].values,
    'True_Sentiment': y_test.values,
    'Predicted_Sentiment': y_pred
})

# Display example predictions
print(example_predictions.head())

example_predictions[['True_Sentiment', 'Predicted_Sentiment']].value_counts()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Display performance metrics
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

# Generate and display confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(conf_matrix)



from imblearn.over_sampling import SMOTE

# Separate features and target variable
X = reviews_df_cleaned['Processed_Text']
y = reviews_df_cleaned['Sentiment']

# Vectorize text data
vectorizer = TfidfVectorizer(max_features=5000)
X_vectorized = vectorizer.fit_transform(X)

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_vectorized, y)

# Split resampled data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Train and evaluate the model again
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

import numpy as np

# Get example predictions along with true sentiments
example_indices = np.random.choice(X_test.shape[0], size=70000, replace=False)
example_predictions = pd.DataFrame({
    'Processed_Text': reviews_df_cleaned.iloc[X_test.indices[example_indices]]['Processed_Text'].values,
    'True_Sentiment': y_test.iloc[example_indices].values,
    'Predicted_Sentiment': y_pred[example_indices]
})

# Display example predictions
print(example_predictions)

example_predictions[['True_Sentiment', 'Predicted_Sentiment']].value_counts()



# Train Logistic Regression model with class weights
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train, y_train) #using resampled ones with SMOTE

# Predictions and evaluation
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))



# weight balancing without SMOTE
# Vectorize the text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X = tfidf_vectorizer.fit_transform(reviews_df_cleaned['Processed_Text'])
y = reviews_df_cleaned['Sentiment']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression model with class weights
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))



# Original indices of the test set
test_indices = y_test.index

# Fetch the corresponding processed text using the original indices
example_predictions = pd.DataFrame({
    'Processed_Text': reviews_df_cleaned.loc[test_indices, 'Processed_Text'].values,
    'True_Sentiment': y_test.values,
    'Predicted_Sentiment': y_pred
})

# Display example predictions
print(example_predictions.head())

example_predictions[['True_Sentiment', 'Predicted_Sentiment']].value_counts()

